{"pageProps":{"mappings":[{"header":{"label":"연구","path":"00.research","md":"","preview":true,"sub":[{"label":"베이지안 추론","path":"bayesian-inference","md":"","preview":true,"sub":[{"label":"Coin Tossing","path":"coin-tossing","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/coin-tossing/coin-tossing.jpg","snippet":"동전을 던졌을 때, 앞면이 나올 확률을 데이터에 기반하여 추정해보자","depth":2},{"label":"Curve Fitting","path":"curve-fitting","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/curve-fitting/curve-fitting.JPG","snippet":"가지 중요한 개념을 설명하기에 앞서, 간단한 회귀(Regression) 문제를 소개해 보도록 하겠다.","depth":2},{"label":"Gibbs Sampling","path":"gibbs-sampling","md":"","preview":true,"sub":[],"img":"/assets/study/inverse-transform-sampling/Inverse_Transform_Sampling_Example.gif","snippet":"Gibbs Sampling을 구현하기위해 사용한 Inverse Transform Sampling 기법을 소개하면서 실질적인 구현 방법을 먼저 소개하고, 이론적인 배경은 나중에 업데이트 할 예정이다.","depth":2},{"label":"Variational Inference","path":"variational-inference","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Inference는 [Bayeisan Inference](/docs/research/bayesian-inference)에서 적용되는 테크닉으로 개인적으로는 상당히 공부하기 어려웠던 것 중 하나여서 시간을 내어 정리해 보려고 한다.","depth":2}],"img":"/barcode.png","snippet":"Inference를 설명하기에 앞서 다음과 같은 순서로 각 개념을 이해하는 것이 중요하다.","depth":1},{"label":"가우시안 혼합 모델","path":"gaussian-mixture-model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"K-means Clustering","path":"k-means","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"K-means 알고리즘은 Gaussian Mixture Model의 특별한 경우이다. 그리고 EM 알고리즘의 Expectation 단계와 Maximazation 단계를 거쳐 학습하는 과정을 거친다.","depth":1},{"label":"Multi-Armed Bandit","path":"multi-armed-bandit","md":"","preview":true,"sub":[],"img":"/assets/research/multi-armed-bandit/mab.JPG","snippet":"여러대의 슬롯 머신이 있다고 하자. 그리고 일확천금을 위해서 어떤 사람이 슬롯 머신을 여기 저기서 당기고 있다. 이때 이 사람이 수익을 극대화 하는 방법이 있을까?","depth":1},{"label":"PageRank","path":"pagerank","md":"","preview":true,"sub":[],"img":"/assets/research/pagerank/pagerank.png","snippet":"상당히 직관적이고 간단하게 이해할 수 있는 개념이지만 그 이면을 들여다 보면 공부할 만한 사실들이 상당히 많이 있다. 그 중 중요하다고 생각하는 부분들에 대해서 소개하려고 한다.","depth":1},{"label":"추천 시스템","path":"recommendation-system","md":"","preview":true,"sub":[{"label":"컨텐츠 기반 알고리즘","path":"contents","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Matrix Factorization","path":"matrix-factorization","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Factorization은 추천 시스템에서 협업 필터링(Collaborative Filtering) 알고리즘에 속한다. 아이디어는 상당히 간단한데 User와 Item을 행과 열로 가진 Matrix 분해햐여 User와 Item을 low dimensional latent space에 사상 시키는 방법이다. 이를 위해 아랴와 같이 크게 두가지 방식으로 User-Item Matrix를 Decomposition 할 수 있다.","depth":2},{"label":"모델 기반 협업 필터링","path":"model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"neighbor","path":"neighbor","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"Stochastic Process","path":"stochastic-process","md":"","preview":true,"sub":[{"label":"디리클레 프로세스","path":"dirichlet-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"가우시안 프로세스","path":"gaussian-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"혹스 프로세스","path":"hawkes-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"포아송 프로세스","path":"poisson-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"Stochastic Process란, Random Variable(확률 변수) 혹은 function의 collection을 의미한다.","depth":1},{"label":"Singular Value Decomposition","path":"svd","md":"","preview":true,"sub":[],"img":"/assets/research/svd/axis.JPG","snippet":"Singular Value Decomposition (이하 SVD)는 Eigendecomposition의 일반화된 형태이므로 먼저 Eigendecompositon에 대해 정리해 본다.","depth":1},{"label":"Topic Model","path":"topic-modeling","md":"","preview":true,"sub":[{"label":"Adversarial-neural Event Model","path":"aem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Correlated Topic Model","path":"ctm","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Gaussain LDA","path":"glda","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Hierarchical Dirichlet Process","path":"hdp","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"LDA의 Non-parametric 버전으로 토픽 갯수 K를 지정하지 않아도 되는 더 일반적인 모델","depth":2},{"label":"Latent Dirichlet Allocation","path":"lda","md":"","preview":true,"sub":[],"img":"/assets/research/topic_modeling/lda/dist_desc.JPG","snippet":"LDA는 임의의 문서를 K개의 토픽 분포로 표현하고, 각 토픽은 V개의 단어 분포로 표현하는 모델이다.","depth":2},{"label":"Latent Event Model","path":"lem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"토픽 모델이 뭔지 정리해보자","depth":1},{"label":"Variational AutoEncoder","path":"variational-autoencoder","md":"","preview":true,"sub":[],"img":"/assets/research/variational-autoencoder/ae-vae.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"공부했던 것들 중에, 생각할 것들이 많았던 것들을 정리하고 있다.","depth":0},"side_bar":[{"label":"베이지안 추론","path":"bayesian-inference","md":"","preview":true,"sub":[{"label":"Coin Tossing","path":"coin-tossing","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/coin-tossing/coin-tossing.jpg","snippet":"동전을 던졌을 때, 앞면이 나올 확률을 데이터에 기반하여 추정해보자","depth":2},{"label":"Curve Fitting","path":"curve-fitting","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/curve-fitting/curve-fitting.JPG","snippet":"가지 중요한 개념을 설명하기에 앞서, 간단한 회귀(Regression) 문제를 소개해 보도록 하겠다.","depth":2},{"label":"Gibbs Sampling","path":"gibbs-sampling","md":"","preview":true,"sub":[],"img":"/assets/study/inverse-transform-sampling/Inverse_Transform_Sampling_Example.gif","snippet":"Gibbs Sampling을 구현하기위해 사용한 Inverse Transform Sampling 기법을 소개하면서 실질적인 구현 방법을 먼저 소개하고, 이론적인 배경은 나중에 업데이트 할 예정이다.","depth":2},{"label":"Variational Inference","path":"variational-inference","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Inference는 [Bayeisan Inference](/docs/research/bayesian-inference)에서 적용되는 테크닉으로 개인적으로는 상당히 공부하기 어려웠던 것 중 하나여서 시간을 내어 정리해 보려고 한다.","depth":2}],"img":"/barcode.png","snippet":"Inference를 설명하기에 앞서 다음과 같은 순서로 각 개념을 이해하는 것이 중요하다.","depth":1},{"label":"가우시안 혼합 모델","path":"gaussian-mixture-model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"K-means Clustering","path":"k-means","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"K-means 알고리즘은 Gaussian Mixture Model의 특별한 경우이다. 그리고 EM 알고리즘의 Expectation 단계와 Maximazation 단계를 거쳐 학습하는 과정을 거친다.","depth":1},{"label":"Multi-Armed Bandit","path":"multi-armed-bandit","md":"","preview":true,"sub":[],"img":"/assets/research/multi-armed-bandit/mab.JPG","snippet":"여러대의 슬롯 머신이 있다고 하자. 그리고 일확천금을 위해서 어떤 사람이 슬롯 머신을 여기 저기서 당기고 있다. 이때 이 사람이 수익을 극대화 하는 방법이 있을까?","depth":1},{"label":"PageRank","path":"pagerank","md":"","preview":true,"sub":[],"img":"/assets/research/pagerank/pagerank.png","snippet":"상당히 직관적이고 간단하게 이해할 수 있는 개념이지만 그 이면을 들여다 보면 공부할 만한 사실들이 상당히 많이 있다. 그 중 중요하다고 생각하는 부분들에 대해서 소개하려고 한다.","depth":1},{"label":"추천 시스템","path":"recommendation-system","md":"","preview":true,"sub":[{"label":"컨텐츠 기반 알고리즘","path":"contents","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Matrix Factorization","path":"matrix-factorization","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Factorization은 추천 시스템에서 협업 필터링(Collaborative Filtering) 알고리즘에 속한다. 아이디어는 상당히 간단한데 User와 Item을 행과 열로 가진 Matrix 분해햐여 User와 Item을 low dimensional latent space에 사상 시키는 방법이다. 이를 위해 아랴와 같이 크게 두가지 방식으로 User-Item Matrix를 Decomposition 할 수 있다.","depth":2},{"label":"모델 기반 협업 필터링","path":"model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"neighbor","path":"neighbor","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"Stochastic Process","path":"stochastic-process","md":"","preview":true,"sub":[{"label":"디리클레 프로세스","path":"dirichlet-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"가우시안 프로세스","path":"gaussian-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"혹스 프로세스","path":"hawkes-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"포아송 프로세스","path":"poisson-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"Stochastic Process란, Random Variable(확률 변수) 혹은 function의 collection을 의미한다.","depth":1},{"label":"Singular Value Decomposition","path":"svd","md":"","preview":true,"sub":[],"img":"/assets/research/svd/axis.JPG","snippet":"Singular Value Decomposition (이하 SVD)는 Eigendecomposition의 일반화된 형태이므로 먼저 Eigendecompositon에 대해 정리해 본다.","depth":1},{"label":"Topic Model","path":"topic-modeling","md":"","preview":true,"sub":[{"label":"Adversarial-neural Event Model","path":"aem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Correlated Topic Model","path":"ctm","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Gaussain LDA","path":"glda","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Hierarchical Dirichlet Process","path":"hdp","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"LDA의 Non-parametric 버전으로 토픽 갯수 K를 지정하지 않아도 되는 더 일반적인 모델","depth":2},{"label":"Latent Dirichlet Allocation","path":"lda","md":"","preview":true,"sub":[],"img":"/assets/research/topic_modeling/lda/dist_desc.JPG","snippet":"LDA는 임의의 문서를 K개의 토픽 분포로 표현하고, 각 토픽은 V개의 단어 분포로 표현하는 모델이다.","depth":2},{"label":"Latent Event Model","path":"lem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"토픽 모델이 뭔지 정리해보자","depth":1},{"label":"Variational AutoEncoder","path":"variational-autoencoder","md":"","preview":true,"sub":[],"img":"/assets/research/variational-autoencoder/ae-vae.png","snippet":"","depth":1}]},{"header":{"label":"개발","path":"01.development","md":"","preview":true,"sub":[{"label":"엘라스틱서치","path":"elasticsearch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"사용했던 것들을 정리해보자.","depth":1},{"label":"개발 환경 구축","path":"env","md":"","preview":true,"sub":[{"label":"code-server","path":"code-server","md":"","preview":true,"sub":[],"img":"/assets/development/env/code-server/code-server.png","snippet":"+ ubuntu 20.04","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"명령어 기록","path":"etc","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"sh","depth":1},{"label":"하둡","path":"hadoop","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"java","path":"java","md":"","preview":true,"sub":[{"label":"자바의 비동기 기술","path":"async","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Future","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"쿠버네티스","path":"k8s","md":"","preview":true,"sub":[{"label":"쿠버네티스 설치","path":"installation","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"집에 놀고 있는 리눅스 머신에 K8S를 설치 해보자","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"메시지 브로커","path":"kafka","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"보내고, 처리하고, 삭제한다.","depth":1},{"label":"코틀린","path":"kotlin","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"루씬","path":"lucene","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"거의 Elasticsearch를 이용해서 프로젝트를 진행하지만, 검색이 필요한 경우 Lucene을 이용해서 개발하는 경우가 많았던 것 같다. JAVA에 Lucence 의존성만 추가하면 뭔가 가볍게 시작할 수 있었기 때문인데 점점 기능이 복잡해 질 수록 Elasticsearch가 얼마나 잘 만들어져 있는 것인가를 느끼고 있다. 그래도 Elasticsearch는 Lucence을 가져다 쓰는거니까 먼저 간단한 것 부터 정리해 볼 계획이다.","depth":1},{"label":"리액트","path":"react","md":"","preview":true,"sub":[{"label":"package.json","path":"package-json","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"dependencies vs devDependencies","depth":2},{"label":"react-snap으로 정적페이지 빌드","path":"react-snap","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Basic usage with create-react-app","depth":2},{"label":"GitHub Pages에 SPA","path":"spa-github-pages","md":"","preview":true,"sub":[],"img":"/assets/development/react/spa-github-pages/github-pages-404.JPG","snippet":"1. 문제 상황","depth":2}],"img":"/assets/development/react/react-app.png","snippet":"React 개발 환경 구축","depth":1},{"label":"스프링 부트","path":"spring-boot","md":"","preview":true,"sub":[{"label":"Spring Boot에서 HTTPS 적용","path":"https","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Certificate 만들기","depth":2},{"label":"Maven에서 Spring Boot 설정","path":"maven-support","md":"","preview":true,"sub":[],"img":"/assets/development/spring-boot/maven-support/multi-module.JPG","snippet":"Maven multi-module 프로젝트에서 Spring Boot Application을 Maven Dependency로 Import하기","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"웹플럭스","path":"webflux","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"","depth":0},"side_bar":[{"label":"엘라스틱서치","path":"elasticsearch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"사용했던 것들을 정리해보자.","depth":1},{"label":"개발 환경 구축","path":"env","md":"","preview":true,"sub":[{"label":"code-server","path":"code-server","md":"","preview":true,"sub":[],"img":"/assets/development/env/code-server/code-server.png","snippet":"+ ubuntu 20.04","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"명령어 기록","path":"etc","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"sh","depth":1},{"label":"하둡","path":"hadoop","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"java","path":"java","md":"","preview":true,"sub":[{"label":"자바의 비동기 기술","path":"async","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Future","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"쿠버네티스","path":"k8s","md":"","preview":true,"sub":[{"label":"쿠버네티스 설치","path":"installation","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"집에 놀고 있는 리눅스 머신에 K8S를 설치 해보자","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"메시지 브로커","path":"kafka","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"보내고, 처리하고, 삭제한다.","depth":1},{"label":"코틀린","path":"kotlin","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"루씬","path":"lucene","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"거의 Elasticsearch를 이용해서 프로젝트를 진행하지만, 검색이 필요한 경우 Lucene을 이용해서 개발하는 경우가 많았던 것 같다. JAVA에 Lucence 의존성만 추가하면 뭔가 가볍게 시작할 수 있었기 때문인데 점점 기능이 복잡해 질 수록 Elasticsearch가 얼마나 잘 만들어져 있는 것인가를 느끼고 있다. 그래도 Elasticsearch는 Lucence을 가져다 쓰는거니까 먼저 간단한 것 부터 정리해 볼 계획이다.","depth":1},{"label":"리액트","path":"react","md":"","preview":true,"sub":[{"label":"package.json","path":"package-json","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"dependencies vs devDependencies","depth":2},{"label":"react-snap으로 정적페이지 빌드","path":"react-snap","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Basic usage with create-react-app","depth":2},{"label":"GitHub Pages에 SPA","path":"spa-github-pages","md":"","preview":true,"sub":[],"img":"/assets/development/react/spa-github-pages/github-pages-404.JPG","snippet":"1. 문제 상황","depth":2}],"img":"/assets/development/react/react-app.png","snippet":"React 개발 환경 구축","depth":1},{"label":"스프링 부트","path":"spring-boot","md":"","preview":true,"sub":[{"label":"Spring Boot에서 HTTPS 적용","path":"https","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Certificate 만들기","depth":2},{"label":"Maven에서 Spring Boot 설정","path":"maven-support","md":"","preview":true,"sub":[],"img":"/assets/development/spring-boot/maven-support/multi-module.JPG","snippet":"Maven multi-module 프로젝트에서 Spring Boot Application을 Maven Dependency로 Import하기","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"웹플럭스","path":"webflux","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]},{"header":{"label":"아무거나 정리","path":"02.study","md":"","preview":true,"sub":[{"label":"Chi-Square Test","path":"chi-square-test","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"검정은 하나 이상의 카테고리에서 관측된 빈도와 기대되는 빈도가 통계적으로 유의하게 다른지 검증하는 기법으로, 카이 제곱 분포에 기초한 통계적 가설 검정 방법이다.","depth":1},{"label":"HTTPS와 공개 키 암호 방식","path":"crypto","md":"","preview":true,"sub":[],"img":"/assets/study/crypto/https.png","snippet":"그린 그림인지 모르겠지만 가장 쉽게 잘 설명해주신 것 같다.","depth":1},{"label":"NumPy","path":"numpy","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"np.array","depth":1},{"label":"P-value","path":"p-value","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"pandas","path":"pandas","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"PyTorch","path":"pytorch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"궁금해서 찾아본 것, 알고 있었는데 까먹고 있었던 것, 생각 날 때마다 정리해보자.","depth":0},"side_bar":[{"label":"Chi-Square Test","path":"chi-square-test","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"검정은 하나 이상의 카테고리에서 관측된 빈도와 기대되는 빈도가 통계적으로 유의하게 다른지 검증하는 기법으로, 카이 제곱 분포에 기초한 통계적 가설 검정 방법이다.","depth":1},{"label":"HTTPS와 공개 키 암호 방식","path":"crypto","md":"","preview":true,"sub":[],"img":"/assets/study/crypto/https.png","snippet":"그린 그림인지 모르겠지만 가장 쉽게 잘 설명해주신 것 같다.","depth":1},{"label":"NumPy","path":"numpy","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"np.array","depth":1},{"label":"P-value","path":"p-value","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"pandas","path":"pandas","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"PyTorch","path":"pytorch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]},{"header":{"label":"토이 프로젝트","path":"03.project","md":"","preview":true,"sub":[{"label":"CNN 기반 형태소 분석기","path":"cnn-morph","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"","depth":0},"side_bar":[{"label":"CNN 기반 형태소 분석기","path":"cnn-morph","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]}],"visitors":{"today":1,"total":54},"post":"# Gibbs Sampling\r\n> Gibbs Sampling을 구현하기위해 사용한 Inverse Transform Sampling 기법을 소개하면서 실질적인 구현 방법을 먼저 소개하고, 이론적인 배경은 나중에 업데이트 할 예정이다.\r\n\r\n## Inverse Transform Sampling\r\n\r\n[LDA](/docs/research/topic-modeling/lda)에서 다음과 같은 Sampling Equation을 유도 했는데, 이걸 어떻게 샘플링을 해야하는지 알아내는 데 굉장히 오랜 시간이 걸렸다.\r\n\r\n$$\r\n\\begin{aligned}\r\nP(Z_{(m,n)} = v|Z_{-(m,n)}, W;\\alpha,\\beta)\r\n&\\propto\r\n\\bigg( n_{m,(\\cdot)}^{k,-(m,n)} + \\alpha_{k} \\bigg) \\cfrac { \\Gamma \\bigg( n_{(\\cdot),v}^{k,-(m,n)} + \\beta_{v} \\bigg) } {\\sum_{r=1}^{V}  n_{(\\cdot),r}^{k,-(m,n)} + \\beta_{r}}\r\n\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 1: LDA sampling equation</b>\r\n</figcaption>\r\n\r\n공부를 하다가 어느날 우연히 찾아낸 방법이 바로 inverse transform sampling이라는 것이 었는데, [위키피디아](https://en.wikipedia.org/wiki/Inverse_transform_sampling)에 자세한 설명이 나와 있지만, 간단하게 핵심 아이디어만 소개해 보겠다.\r\n\r\nInverse transform sampling은 다음의 절차를 따른다.\r\n\r\n1. 샘플을 얻고 싶은 확률 분포(예: LDA의 sampling equation)로부터 누적 확률 분포(CDF: Cumulative Distribution Fuction), $$Y = F_X(x) = Pr(X \\leqq x)$$를구한다. (우리가 얻고 싶은 샘플이 바로 이 $$x$$이다,\r\n2. CDF의 역함수, $$ X=F_X^{-1}(y)$$ 를 구한다. 이때 $$Y$$는 누적확률분포이므로 $$0 \\leqq y \\leqq 1$$ 이다)\r\n3. 균등 분포 $$U ~ Unif[0, 1]$$로 부터 샘플 $$u$$를 생성한다. (난수 생성)\r\n4. 2에서 구한 역함수에 u를 대입한 $$ F_X^{-1}(u)$$가 바로 우리가 구하고자하는 샘플 $$x$$가 된다.\r\n\r\n위 방식에 대한 간단한 증명법이 있지만, 그것 보다는 아래 그림을 통해 이 방법이 타당하다는 것을 직관적으로 알 수 있다.\r\n\r\n<img width=\"400\" src=\"/assets/study/inverse-transform-sampling/Inverse_Transform_Sampling_Example.gif\" />\r\n<figcaption align=\"center\">\r\n  <b>그림1: Inverse transform sampling으로 정규 분포 만들기 (출처: 위키피디아)</b>\r\n</figcaption>\r\n\r\n위 그림에서 y축 위에 쌓이는 데이터들은 난수 생성으로 얻을 수 있는 균등 분포의 샘플이고, 이 샘플 각각에 대응하는 x축 위의 값이 우리가 얻고자 하는 목표 분포의 샘플이라고 할 수 있다. 실제 위 그림은 정규 분포의 누적확률분포를 이용한 샘플링을 시뮬레이션 한 것이다.\r\n\r\n물론 누적 분포 구하는 것 자체에 적분이 수반되기 때문에 computation 관점에서 어려울 수 있고, 역함수를 구하는 과정이 해석적이지 않을 수 있지만, 적어도 LDA경우는 discrete한 값인 $$z$$(topic indicator)를 샘플링하는 것은 가능하다. (실제로 나중에 설명할 Correlated Topic Model을 Gibbs Sampling으로 구현하기 위해서는 다른 샘플링 전략이 필요하다.)\r\n\r\n```java\r\nint newTopic = -1;\r\ndouble[] probs = new double[K];\r\nfor (int k = 0; k < K; k++) {\r\n    double prob = calc.calculateProb(document, word, k); // prob. by sampling equation\r\n    probs[k] = prob;\r\n}\r\nfor(int k=1; k<K ;k++) {\r\n    probs[k] += probs[k-1]; // cdf.\r\n}\r\ndouble u = Math.random() * probs[K-1];\r\nfor(newTopic = 0; newTopic<K; newTopic++) {\r\n    if(probs[newTopic] > u)\r\n        break;\r\n}\r\n```\r\n\r\n<figcaption align=\"center\">\r\n  <b>Inverse transform sampling 구현</b>\r\n</figcaption>\r\n\r\n위 코드는 내가 LDA를 구현하면서 Invere transform sampling의 방법을 적용한 것인데, 앞서 설명한 아이디어를 이용해 코드가 구현되어 있는 것을 알 수 있다.\r\n\r\n이 방법을 알게된 이후 부터, Gibbs Sampling 방식으로 베이지안 모델을 구현하는데 속도가 붙었었고, 개인적으로 아래와 같은 모델을 이 방식을 이용하여 직접 구현해 보면서 많은 공부가 되었다.\r\n\r\n1. [Latent Dirichlet Allocatio (LDA)](/docs/research/topic-modeling/lda)\r\n2. [Hierarchical Dirichlet Process (HDP)](/docs/research/topic-modeling/hdp)\r\n3. [Gaussain LDA (GLDA)](/docs/research/topic-modeling/glda)\r\n4. [Correlated Topic Model (CTM)](/docs/research/topic-modeling/ctm)\r\n\r\n한 가지 주의해야 할 것은, Gibbs Samping 방식으로 추론을 하기 위해서는 위와 같은 Sampling Equation을 유도해야하는데, LDA에서 소개했듯이 만만치 않다. (LDA가 그중 가장 쉬운거라고 생각하면됨)\r\n\r\n> 일반적으로는 Prior를 선택할 때, Conjugate Prior를 선택해야 유도가 가능하다\r\n\r\n특히 위 4번 CTM은 LDA 논문을 쓴 저자가 후속 연구로 냈던 방법인데, 저자는 Gibbs Sampling이 아닌 Variational Inference 계열의 방식을 택했다. 왜냐하면 해당 모델에서 사용하는 확률 분포가 conjugate한 관계가 아니었기 때문인데, 이걸 또 Gibbs Sampling으로 구현하기 위한 연구자가 있었고, 나는 이 방식을 이용하여 CTM을 구현했다. 굉장히 수학적인 내용이 많아서 고생을 많이 했지만, 시간을 내서 정리할 계획이다. (4번 구현이 가장 어려웠고 3번은 개념 자체가 어려웠음)\r\n\r\n또한 [Variational Inference](/docs/research/bayesian-inference/variational-inference)에 대해서도 소개를 하려고 한다. (이것도 어려움)\r\n","path":"00.research/bayesian-inference/gibbs-sampling"},"__N_SSG":true}