{"pageProps":{"mappings":[{"header":{"label":"연구","path":"00.research","md":"","preview":true,"sub":[{"label":"베이지안 추론","path":"bayesian-inference","md":"","preview":true,"sub":[{"label":"Coin Tossing","path":"coin-tossing","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/coin-tossing/coin-tossing.jpg","snippet":"동전을 던졌을 때, 앞면이 나올 확률을 데이터에 기반하여 추정해보자","depth":2},{"label":"Curve Fitting","path":"curve-fitting","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/curve-fitting/curve-fitting.JPG","snippet":"가지 중요한 개념을 설명하기에 앞서, 간단한 회귀(Regression) 문제를 소개해 보도록 하겠다.","depth":2},{"label":"Gibbs Sampling","path":"gibbs-sampling","md":"","preview":true,"sub":[],"img":"/assets/study/inverse-transform-sampling/Inverse_Transform_Sampling_Example.gif","snippet":"Gibbs Sampling을 구현하기위해 사용한 Inverse Transform Sampling 기법을 소개하면서 실질적인 구현 방법을 먼저 소개하고, 이론적인 배경은 나중에 업데이트 할 예정이다.","depth":2},{"label":"Variational Inference","path":"variational-inference","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Inference는 [Bayeisan Inference](/docs/research/bayesian-inference)에서 적용되는 테크닉으로 개인적으로는 상당히 공부하기 어려웠던 것 중 하나여서 시간을 내어 정리해 보려고 한다.","depth":2}],"img":"/barcode.png","snippet":"Inference를 설명하기에 앞서 다음과 같은 순서로 각 개념을 이해하는 것이 중요하다.","depth":1},{"label":"가우시안 혼합 모델","path":"gaussian-mixture-model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"K-means Clustering","path":"k-means","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"K-means 알고리즘은 Gaussian Mixture Model의 특별한 경우이다. 그리고 EM 알고리즘의 Expectation 단계와 Maximazation 단계를 거쳐 학습하는 과정을 거친다.","depth":1},{"label":"Multi-Armed Bandit","path":"multi-armed-bandit","md":"","preview":true,"sub":[],"img":"/assets/research/multi-armed-bandit/mab.JPG","snippet":"여러대의 슬롯 머신이 있다고 하자. 그리고 일확천금을 위해서 어떤 사람이 슬롯 머신을 여기 저기서 당기고 있다. 이때 이 사람이 수익을 극대화 하는 방법이 있을까?","depth":1},{"label":"PageRank","path":"pagerank","md":"","preview":true,"sub":[],"img":"/assets/research/pagerank/pagerank.png","snippet":"상당히 직관적이고 간단하게 이해할 수 있는 개념이지만 그 이면을 들여다 보면 공부할 만한 사실들이 상당히 많이 있다. 그 중 중요하다고 생각하는 부분들에 대해서 소개하려고 한다.","depth":1},{"label":"추천 시스템","path":"recommendation-system","md":"","preview":true,"sub":[{"label":"컨텐츠 기반 알고리즘","path":"contents","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"생각나는 대로 정리","depth":2},{"label":"Matrix Factorization","path":"matrix-factorization","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Factorization은 추천 시스템에서 협업 필터링(Collaborative Filtering) 알고리즘에 속한다. 아이디어는 상당히 간단한데 User와 Item을 행과 열로 가진 Matrix 분해햐여 User와 Item을 low dimensional latent space에 사상 시키는 방법이다. 이를 위해 아랴와 같이 크게 두가지 방식으로 User-Item Matrix를 Decomposition 할 수 있다.","depth":2},{"label":"모델 기반 협업 필터링","path":"model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"적어 보자","depth":2},{"label":"이웃기반 협업필터링","path":"neighbor","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"일단 생각나는대로 적어보자","depth":2}],"img":"/barcode.png","snippet":"추천 쪽을 실무에서 할 기회는 없었지만, 기본적인 것들에 대해서는 개념적 어렵지는 않은 것 같다","depth":1},{"label":"Stochastic Process","path":"stochastic-process","md":"","preview":true,"sub":[{"label":"디리클레 프로세스","path":"dirichlet-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"가우시안 프로세스","path":"gaussian-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"혹스 프로세스","path":"hawkes-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"포아송 프로세스","path":"poisson-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"Stochastic Process란, Random Variable(확률 변수) 혹은 function의 collection을 의미한다.","depth":1},{"label":"Singular Value Decomposition","path":"svd","md":"","preview":true,"sub":[],"img":"/assets/research/svd/axis.JPG","snippet":"Singular Value Decomposition (이하 SVD)는 Eigendecomposition의 일반화된 형태이므로 먼저 Eigendecompositon에 대해 정리해 본다.","depth":1},{"label":"Topic Model","path":"topic-modeling","md":"","preview":true,"sub":[{"label":"Adversarial-neural Event Model","path":"aem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Correlated Topic Model","path":"ctm","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Gaussain LDA","path":"glda","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Hierarchical Dirichlet Process","path":"hdp","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"LDA의 Non-parametric 버전으로 토픽 갯수 K를 지정하지 않아도 되는 더 일반적인 모델","depth":2},{"label":"Latent Dirichlet Allocation","path":"lda","md":"","preview":true,"sub":[],"img":"/assets/research/topic_modeling/lda/dist_desc.JPG","snippet":"LDA는 임의의 문서를 K개의 토픽 분포로 표현하고, 각 토픽은 V개의 단어 분포로 표현하는 모델이다.","depth":2},{"label":"Latent Event Model","path":"lem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"토픽 모델이 뭔지 정리해보자","depth":1},{"label":"Variational AutoEncoder","path":"variational-autoencoder","md":"","preview":true,"sub":[],"img":"/assets/research/variational-autoencoder/ae-vae.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"공부했던 것들 중에, 생각할 것들이 많았던 것들을 정리하고 있다.","depth":0},"side_bar":[{"label":"베이지안 추론","path":"bayesian-inference","md":"","preview":true,"sub":[{"label":"Coin Tossing","path":"coin-tossing","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/coin-tossing/coin-tossing.jpg","snippet":"동전을 던졌을 때, 앞면이 나올 확률을 데이터에 기반하여 추정해보자","depth":2},{"label":"Curve Fitting","path":"curve-fitting","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/curve-fitting/curve-fitting.JPG","snippet":"가지 중요한 개념을 설명하기에 앞서, 간단한 회귀(Regression) 문제를 소개해 보도록 하겠다.","depth":2},{"label":"Gibbs Sampling","path":"gibbs-sampling","md":"","preview":true,"sub":[],"img":"/assets/study/inverse-transform-sampling/Inverse_Transform_Sampling_Example.gif","snippet":"Gibbs Sampling을 구현하기위해 사용한 Inverse Transform Sampling 기법을 소개하면서 실질적인 구현 방법을 먼저 소개하고, 이론적인 배경은 나중에 업데이트 할 예정이다.","depth":2},{"label":"Variational Inference","path":"variational-inference","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Inference는 [Bayeisan Inference](/docs/research/bayesian-inference)에서 적용되는 테크닉으로 개인적으로는 상당히 공부하기 어려웠던 것 중 하나여서 시간을 내어 정리해 보려고 한다.","depth":2}],"img":"/barcode.png","snippet":"Inference를 설명하기에 앞서 다음과 같은 순서로 각 개념을 이해하는 것이 중요하다.","depth":1},{"label":"가우시안 혼합 모델","path":"gaussian-mixture-model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"K-means Clustering","path":"k-means","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"K-means 알고리즘은 Gaussian Mixture Model의 특별한 경우이다. 그리고 EM 알고리즘의 Expectation 단계와 Maximazation 단계를 거쳐 학습하는 과정을 거친다.","depth":1},{"label":"Multi-Armed Bandit","path":"multi-armed-bandit","md":"","preview":true,"sub":[],"img":"/assets/research/multi-armed-bandit/mab.JPG","snippet":"여러대의 슬롯 머신이 있다고 하자. 그리고 일확천금을 위해서 어떤 사람이 슬롯 머신을 여기 저기서 당기고 있다. 이때 이 사람이 수익을 극대화 하는 방법이 있을까?","depth":1},{"label":"PageRank","path":"pagerank","md":"","preview":true,"sub":[],"img":"/assets/research/pagerank/pagerank.png","snippet":"상당히 직관적이고 간단하게 이해할 수 있는 개념이지만 그 이면을 들여다 보면 공부할 만한 사실들이 상당히 많이 있다. 그 중 중요하다고 생각하는 부분들에 대해서 소개하려고 한다.","depth":1},{"label":"추천 시스템","path":"recommendation-system","md":"","preview":true,"sub":[{"label":"컨텐츠 기반 알고리즘","path":"contents","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"생각나는 대로 정리","depth":2},{"label":"Matrix Factorization","path":"matrix-factorization","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Factorization은 추천 시스템에서 협업 필터링(Collaborative Filtering) 알고리즘에 속한다. 아이디어는 상당히 간단한데 User와 Item을 행과 열로 가진 Matrix 분해햐여 User와 Item을 low dimensional latent space에 사상 시키는 방법이다. 이를 위해 아랴와 같이 크게 두가지 방식으로 User-Item Matrix를 Decomposition 할 수 있다.","depth":2},{"label":"모델 기반 협업 필터링","path":"model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"적어 보자","depth":2},{"label":"이웃기반 협업필터링","path":"neighbor","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"일단 생각나는대로 적어보자","depth":2}],"img":"/barcode.png","snippet":"추천 쪽을 실무에서 할 기회는 없었지만, 기본적인 것들에 대해서는 개념적 어렵지는 않은 것 같다","depth":1},{"label":"Stochastic Process","path":"stochastic-process","md":"","preview":true,"sub":[{"label":"디리클레 프로세스","path":"dirichlet-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"가우시안 프로세스","path":"gaussian-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"혹스 프로세스","path":"hawkes-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"포아송 프로세스","path":"poisson-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"Stochastic Process란, Random Variable(확률 변수) 혹은 function의 collection을 의미한다.","depth":1},{"label":"Singular Value Decomposition","path":"svd","md":"","preview":true,"sub":[],"img":"/assets/research/svd/axis.JPG","snippet":"Singular Value Decomposition (이하 SVD)는 Eigendecomposition의 일반화된 형태이므로 먼저 Eigendecompositon에 대해 정리해 본다.","depth":1},{"label":"Topic Model","path":"topic-modeling","md":"","preview":true,"sub":[{"label":"Adversarial-neural Event Model","path":"aem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Correlated Topic Model","path":"ctm","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Gaussain LDA","path":"glda","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Hierarchical Dirichlet Process","path":"hdp","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"LDA의 Non-parametric 버전으로 토픽 갯수 K를 지정하지 않아도 되는 더 일반적인 모델","depth":2},{"label":"Latent Dirichlet Allocation","path":"lda","md":"","preview":true,"sub":[],"img":"/assets/research/topic_modeling/lda/dist_desc.JPG","snippet":"LDA는 임의의 문서를 K개의 토픽 분포로 표현하고, 각 토픽은 V개의 단어 분포로 표현하는 모델이다.","depth":2},{"label":"Latent Event Model","path":"lem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"토픽 모델이 뭔지 정리해보자","depth":1},{"label":"Variational AutoEncoder","path":"variational-autoencoder","md":"","preview":true,"sub":[],"img":"/assets/research/variational-autoencoder/ae-vae.png","snippet":"","depth":1}]},{"header":{"label":"개발","path":"01.development","md":"","preview":true,"sub":[{"label":"엘라스틱서치","path":"elasticsearch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"사용했던 것들을 정리해보자.","depth":1},{"label":"개발 환경 구축","path":"env","md":"","preview":true,"sub":[{"label":"code-server","path":"code-server","md":"","preview":true,"sub":[],"img":"/assets/development/env/code-server/code-server.png","snippet":"+ ubuntu 20.04","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"명령어 기록","path":"etc","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"sh","depth":1},{"label":"하둡","path":"hadoop","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"java","path":"java","md":"","preview":true,"sub":[{"label":"자바의 비동기 기술","path":"async","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Future","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"쿠버네티스","path":"k8s","md":"","preview":true,"sub":[{"label":"쿠버네티스 설치","path":"installation","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"집에 놀고 있는 리눅스 머신에 K8S를 설치 해보자","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"메시지 브로커","path":"kafka","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"보내고, 처리하고, 삭제한다.","depth":1},{"label":"코틀린","path":"kotlin","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"루씬","path":"lucene","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"거의 Elasticsearch를 이용해서 프로젝트를 진행하지만, 검색이 필요한 경우 Lucene을 이용해서 개발하는 경우가 많았던 것 같다. JAVA에 Lucence 의존성만 추가하면 뭔가 가볍게 시작할 수 있었기 때문인데 점점 기능이 복잡해 질 수록 Elasticsearch가 얼마나 잘 만들어져 있는 것인가를 느끼고 있다. 그래도 Elasticsearch는 Lucence을 가져다 쓰는거니까 먼저 간단한 것 부터 정리해 볼 계획이다.","depth":1},{"label":"리액트","path":"react","md":"","preview":true,"sub":[{"label":"package.json","path":"package-json","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"dependencies vs devDependencies","depth":2},{"label":"react-snap으로 정적페이지 빌드","path":"react-snap","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Basic usage with create-react-app","depth":2},{"label":"GitHub Pages에 SPA","path":"spa-github-pages","md":"","preview":true,"sub":[],"img":"/assets/development/react/spa-github-pages/github-pages-404.JPG","snippet":"1. 문제 상황","depth":2}],"img":"/assets/development/react/react-app.png","snippet":"React 개발 환경 구축","depth":1},{"label":"스프링 부트","path":"spring-boot","md":"","preview":true,"sub":[{"label":"Spring Boot에서 HTTPS 적용","path":"https","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Certificate 만들기","depth":2},{"label":"Maven에서 Spring Boot 설정","path":"maven-support","md":"","preview":true,"sub":[],"img":"/assets/development/spring-boot/maven-support/multi-module.JPG","snippet":"Maven multi-module 프로젝트에서 Spring Boot Application을 Maven Dependency로 Import하기","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"웹플럭스","path":"webflux","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"","depth":0},"side_bar":[{"label":"엘라스틱서치","path":"elasticsearch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"사용했던 것들을 정리해보자.","depth":1},{"label":"개발 환경 구축","path":"env","md":"","preview":true,"sub":[{"label":"code-server","path":"code-server","md":"","preview":true,"sub":[],"img":"/assets/development/env/code-server/code-server.png","snippet":"+ ubuntu 20.04","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"명령어 기록","path":"etc","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"sh","depth":1},{"label":"하둡","path":"hadoop","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"java","path":"java","md":"","preview":true,"sub":[{"label":"자바의 비동기 기술","path":"async","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Future","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"쿠버네티스","path":"k8s","md":"","preview":true,"sub":[{"label":"쿠버네티스 설치","path":"installation","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"집에 놀고 있는 리눅스 머신에 K8S를 설치 해보자","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"메시지 브로커","path":"kafka","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"보내고, 처리하고, 삭제한다.","depth":1},{"label":"코틀린","path":"kotlin","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"루씬","path":"lucene","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"거의 Elasticsearch를 이용해서 프로젝트를 진행하지만, 검색이 필요한 경우 Lucene을 이용해서 개발하는 경우가 많았던 것 같다. JAVA에 Lucence 의존성만 추가하면 뭔가 가볍게 시작할 수 있었기 때문인데 점점 기능이 복잡해 질 수록 Elasticsearch가 얼마나 잘 만들어져 있는 것인가를 느끼고 있다. 그래도 Elasticsearch는 Lucence을 가져다 쓰는거니까 먼저 간단한 것 부터 정리해 볼 계획이다.","depth":1},{"label":"리액트","path":"react","md":"","preview":true,"sub":[{"label":"package.json","path":"package-json","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"dependencies vs devDependencies","depth":2},{"label":"react-snap으로 정적페이지 빌드","path":"react-snap","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Basic usage with create-react-app","depth":2},{"label":"GitHub Pages에 SPA","path":"spa-github-pages","md":"","preview":true,"sub":[],"img":"/assets/development/react/spa-github-pages/github-pages-404.JPG","snippet":"1. 문제 상황","depth":2}],"img":"/assets/development/react/react-app.png","snippet":"React 개발 환경 구축","depth":1},{"label":"스프링 부트","path":"spring-boot","md":"","preview":true,"sub":[{"label":"Spring Boot에서 HTTPS 적용","path":"https","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Certificate 만들기","depth":2},{"label":"Maven에서 Spring Boot 설정","path":"maven-support","md":"","preview":true,"sub":[],"img":"/assets/development/spring-boot/maven-support/multi-module.JPG","snippet":"Maven multi-module 프로젝트에서 Spring Boot Application을 Maven Dependency로 Import하기","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"웹플럭스","path":"webflux","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]},{"header":{"label":"아무거나 정리","path":"02.study","md":"","preview":true,"sub":[{"label":"Chi-Square Test","path":"chi-square-test","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"검정은 하나 이상의 카테고리에서 관측된 빈도와 기대되는 빈도가 통계적으로 유의하게 다른지 검증하는 기법으로, 카이 제곱 분포에 기초한 통계적 가설 검정 방법이다.","depth":1},{"label":"HTTPS와 공개 키 암호 방식","path":"crypto","md":"","preview":true,"sub":[],"img":"/assets/study/crypto/https.png","snippet":"그린 그림인지 모르겠지만 가장 쉽게 잘 설명해주신 것 같다.","depth":1},{"label":"NumPy","path":"numpy","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"np.array","depth":1},{"label":"P-value","path":"p-value","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"pandas","path":"pandas","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"PyTorch","path":"pytorch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"궁금해서 찾아본 것, 알고 있었는데 까먹고 있었던 것, 생각 날 때마다 정리해보자.","depth":0},"side_bar":[{"label":"Chi-Square Test","path":"chi-square-test","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"검정은 하나 이상의 카테고리에서 관측된 빈도와 기대되는 빈도가 통계적으로 유의하게 다른지 검증하는 기법으로, 카이 제곱 분포에 기초한 통계적 가설 검정 방법이다.","depth":1},{"label":"HTTPS와 공개 키 암호 방식","path":"crypto","md":"","preview":true,"sub":[],"img":"/assets/study/crypto/https.png","snippet":"그린 그림인지 모르겠지만 가장 쉽게 잘 설명해주신 것 같다.","depth":1},{"label":"NumPy","path":"numpy","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"np.array","depth":1},{"label":"P-value","path":"p-value","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"pandas","path":"pandas","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"PyTorch","path":"pytorch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]},{"header":{"label":"토이 프로젝트","path":"03.project","md":"","preview":true,"sub":[{"label":"CNN 기반 형태소 분석기","path":"cnn-morph","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"리액트","path":"react-simple-documentation","md":"","preview":true,"sub":[{"label":"GitHub를 이용한 호스팅","path":"github-hosting","md":"","preview":true,"sub":[],"img":"/assets/project/react-simple-documentation/github-hosting/github-repo.JPG","snippet":"계정이 있다면 자신이 생성한 저장소(Repository)를 통해 자신이 만든 리액트로 만든 블로그를 무료로 호스팅 할 수 있다.","depth":2},{"label":"설치 및 사용 방법","path":"how-to-use-blog","md":"","preview":true,"sub":[],"img":"/assets/project/react-simple-documentation/how-to-use-blog/desktop.JPG","snippet":"내 블로그 템플릿 설치와 사용법","depth":2}],"img":"/barcode.png","snippet":"블로그 시작하려다 블로그 템플릿 만들기","depth":1}],"img":"/barcode.png","snippet":"","depth":0},"side_bar":[{"label":"CNN 기반 형태소 분석기","path":"cnn-morph","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"리액트","path":"react-simple-documentation","md":"","preview":true,"sub":[{"label":"GitHub를 이용한 호스팅","path":"github-hosting","md":"","preview":true,"sub":[],"img":"/assets/project/react-simple-documentation/github-hosting/github-repo.JPG","snippet":"계정이 있다면 자신이 생성한 저장소(Repository)를 통해 자신이 만든 리액트로 만든 블로그를 무료로 호스팅 할 수 있다.","depth":2},{"label":"설치 및 사용 방법","path":"how-to-use-blog","md":"","preview":true,"sub":[],"img":"/assets/project/react-simple-documentation/how-to-use-blog/desktop.JPG","snippet":"내 블로그 템플릿 설치와 사용법","depth":2}],"img":"/barcode.png","snippet":"블로그 시작하려다 블로그 템플릿 만들기","depth":1}]}],"post":"# Latent Dirichlet Allocation (LDA)\r\n\r\n> LDA는 임의의 문서를 K개의 토픽 분포로 표현하고, 각 토픽은 V개의 단어 분포로 표현하는 모델이다.\r\n\r\n> 토픽 갯수 K와 단어집 갯수 V는 하이퍼파라미터이다. 특히 토픽 갯수 K를 지정하지 않는, 좀 더 advanced한 방법론도 존재하는데 [Hierarchical Dirichlet Process](/docs/research/topic-modeling/hdp)에서 소개하도록 하겠다.\r\n\r\n일반적으로 사람들이 글을 쓸 때는 여러가지 소재를 담아 이야기를 구성한다. 이때 이 소재가 바로 토픽모델에서 말하는 토픽(Topic) 이다.\r\n\r\n예를들어, 신문 기자가 \"애플, 아이폰 출시\" 라는 제목으로 기사를 작성한다고 가정해보자. 보통 이런 기사에는 아이폰, iOS, 카메라 등 제품 혹은 기술에 대한 소재, 그리고 생산량, 수요, 판매, 실적 등 어떤 시장 상황과 관련된 소재 등 다양한 소재로 이야기가 구성된다.\r\n\r\n이렇듯 LDA가 문서를 복수개 토픽의 mixture로 표현하고, 각 토픽을 각 토픽을 설명하는 복수개 단어의 mixture로 표한하는 것(mixture of mixture)은 상당히 자연스럽고 합리적인 것 이라고 볼 수 있다.\r\n\r\n<img width=\"600\" src=\"/assets/research/topic_modeling/lda/dist_desc.JPG\" />\r\n<figcaption align=\"center\">\r\n  <b>그림1: 문서에 대한 토픽 분포와 토픽에 대한 단어 분포</b>\r\n</figcaption>\r\n\r\n위 그림은 지금 까지 설명한 내용을 최대한 이해하기 쉽게 표현해 본 것인데, 이를 좀 더 formal하게 나타낸다면 다음과 같은 Graphical Model로 설명 할 수 있다.\r\n\r\n## Graphical Model\r\n\r\n> Graphical Model이란, 확률 모델을 표현하는 그래프로써 확률 변수(Random Variable)간의 상호 의존성을 나타내기 용이하고 주로 베이지안 통계에서 많이 사용된다.\r\n\r\nGraphical Model은 LDA 모델을 직관적으로 이해하는데 도움이 된다. 아래 그림과 같은 표현법을 plate notation이라고 하는데 동그라미는 확률 변수를, 화살표는 변수간 의존성을 의미한다. $$W$$ 동그라미는 특별히 까맣게 색칠되어 있는데 이것은 관측된 것이라는 의미이다.\r\n\r\n> LDA는 비지도 학습 모델(Unsupervised Model)로 모델 학습을 위해 주어지는 것은 텍스트 코퍼스, 즉 문서 내에 있는 단어들 뿐이다.\r\n\r\n가장 큰 사각형은 문서를 의미하는데 전체 M개의 문서가 있다는 것이고 각 문서의 토픽 분포가 $$\\theta$$라는 것을 의미한다. 이 내부에 있는 작은 사각형은 문서에 포함된 단어를 의미하는데 각 문서에는 N개의 단어가 있다는 뜻이고, 각 단어를 $$W$$로 표현하고 이 단어에 대응하는 토픽(assignment 혹은 indicator)은 $$Z$$로 표현하고 있다. 가장 작은 사각형은 K개의 토픽을 의미하는데, 각 토픽의 단어 분포를 $$\\phi$$로 표현하고 있다.\r\n\r\n<img width=\"600\" src=\"/assets/research/topic_modeling/lda/lda-model.png\" />\r\n<figcaption align=\"center\">\r\n  <b>그림 2: Latent Dirichlet Allocation</b>\r\n</figcaption>\r\n\r\n- $$M$$: 전체 문서의 갯수\r\n- $$N$$: 문서 내 단어 갯수 ($$i$$ 문서는 $$N_i$$ 단어를 가짐)\r\n- $$\\alpha$$: 문서 별 토픽 분포에 대한 사전 확률 분포(prior)인 디리클레 분포의 파라미터 ($$K$$ 차원 벡터)\r\n- $$\\beta$$: 토픽 별 단어 분포에 대한 사전 확률 분포(prior)인 디리클레 분포의 파라미터 ($$V$$ 차원 벡터)\r\n- $$\\theta_i$$: 문서 $$i$$의 토픽 분포 ($$K$$ 차원 벡터)\r\n- $$\\phi_k$$: 토픽 $$k$$의 단어 분포 ($$V$$ 차원 벡터)\r\n- $$z_{ij}$$: $$i$$문서의 $$j$$번째 단어의 토픽\r\n- $$w_{ij}$$: $$i$$문서의 $$j$$번째 단어\r\n\r\nGraphical Model과 이어서 설명할 Generative Story를 통해서, LDA가 많은 수의 문서만 이용하여, 어떻게 텍스트의 시멘틱한 구조를 만들어 낼 수 있는지 가늠해 볼 수 있을 것이다.\r\n\r\n## Generative Story\r\n\r\n임의의 문서는 첫번째 단어, 두번째 단어, 세번째 단어, ... 이렇게 한단어 한단어씩 씌여지는데, 각 단어는 어떤 토픽에 대한 멤버쉽을 갖는다.\r\n\r\n다시말하면, 사람이 글을 쓸 때 단어를 하나를 선택하기에 앞서 어떤 토픽을 먼저 정하고, 그 토픽에 맞는 단어를 골라서 쓴다라고 생각을 해야한다. (이렇게 생각해야 LDA를 이해할 수 있다.) 이를 염두해 놓으면 아래에 있는 설명을 좀 더 쉽게 이해할 수 있다.\r\n\r\n**LDA에서는 문서의 발생 과정을 다음과 같이 가정 한다.**\r\n\r\n1. 사전 확률 분포인 디리클레 분포로부터 샘플링하여 $$\\theta_i$$를 얻는다.\r\n   - (그림 2의 초록색 점선)\r\n   - $$\\theta_i \\sim Dir(\\alpha)$$\r\n   - 코퍼스에 총 $$M$$개의 문서가 있으므로 $$\\theta_1$$, ..., $$\\theta_M$$\r\n2. 사전 확률 분포인 디리클레 분포로 부터 샘플링하여 $$\\phi_k$$를 얻는다.\r\n   - (그림 2의 파란색 점선)\r\n   - $$\\phi_k \\sim Dir(\\beta)$$\r\n   - 토픽 갯수를 총 K개로 가정했으므로 $$\\phi_1$$, ..., $$\\phi_K$$\r\n3. 문서 $$i$$의 각 단어가 선정되기 앞서 토픽이 선정되므로 $$\\theta_i$$를 파라미터로 갖는 다항 분포로 부터 토픽 $$z_{ij}$$를 샘플링\r\n   - (그림 2의 주황색 점선)\r\n   - $$z_{ij} \\sim Multinomial(\\theta_i)$$\r\n4. 토픽 $$z_{ij}$$가 선정되면 해당 토픽에서 $$\\phi_{z_{ij}}$$를 파라미터로 갖는 다항 분포로 부터 단어 $$w_{ij}$$를 샘플링\r\n   - (그림 2의 빨간색 점선)\r\n   - $$w_{ij} \\sim Multinomial(\\phi_{z_{ij}})$$\r\n\r\n문서의 토픽 분포, 토픽의 단어 분포 모두 다항 분포를 사용한다. 비유를 들어본다면, 문서가 작성된다는 것은 $$K$$개의 면이 있는 주사위를 던져 토픽을 뽑고, 뽑힌 토픽에 해당하는 ($$V$$개의 면이 있는) 다른 주사위를 던져, 단어를 뽑는 행위를 반복하는 것으로 본다.\r\n\r\n> LDA가 문서를 만들어 내는 것 처럼 어떤 확률적 생성 과정이 가능하려면, 앞서 설명한 주사위가 필요하다. 즉, 각 문서마다 토픽을 뽑을 수 있는 주사위가 필요하고 각 토픽마다 단어를 뽑을 수 있는 주사위가 필요하다. 이러한 주사위를 잘 깍아서 만드는 과정이 LDA를 학습하는 과정이라고 볼 수 있다.\r\n\r\n## 베이지안 추론(Bayesian Inference)\r\n\r\nLDA 모델에서, 소위 말해 학습이라고 하는 것은 관측할 수 있는 값인 $$W$$(단어)를 제외한 나머지를 모두 잠재 변수(Latent Variable)로 생각하고, 이에 대한 사후 확률 분포(Posterior Distribution)를 구하는 것을 말한다. (일단, 이 분포를 알아내는 과정을 베이지안 추론이라고 생각하면 된다.)\r\n\r\n$$\r\n\\begin{aligned}\r\nP(Z,\\Theta,\\Phi|W,\\alpha,\\beta) = \\cfrac {P(W,Z,\\Theta,\\Phi;\\alpha,\\beta)} {P(W|\\alpha,\\beta)}\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 1: 잠재 변수에 대한 사후 확률 분포</b>\r\n</figcaption>\r\n\r\n여기서 우변의 분모는,\r\n\r\n모든 문서-토픽 분포 $$\\Theta$$, 모든 토픽-단어 분포 $$\\Phi$$, 모든 토픽 assignments $$Z$$, 코퍼스의 모든 단어 $$W$$, 그리고 주어진 하이퍼파라미터 $$\\alpha$$, $$\\beta$$ 의 결합 확률 분포이다. 앞서 나왔던 Graphcial Model을 참조하여 아래와 같은 식으로 정리할 수 있다.\r\n\r\n<!-- <img width=\"600\" src=\"/docs/assets/research/topic_modeling/lda/total-prob.png\" />\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 1: 전체 확률 변수에 대한 결합 확률 분포</b>\r\n</figcaption> -->\r\n\r\n$$\r\n\\begin{aligned}\r\nP(W,Z,\\Theta,\\Phi;\\alpha,\\beta) = \\displaystyle\\prod_{i=1}^{K} P(\\phi_i;\\beta)\\displaystyle\\prod_{j=1}^{M} \\Bigg[ P(\\theta_j;\\alpha)    \\displaystyle\\prod_{t=1}^{N} \\bigg[ P(Z_{j,t} | \\theta_j)P(W_{j,t}|\\phi_{z_{jt}}) \\bigg] \\Bigg]\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 2: 전체 확률 변수에 대한 결합 확률 분포</b>\r\n</figcaption>\r\n\r\n그리고 우변의 분자는,\r\n\r\n위 결합 확률 분포를 $$\\Phi$$, $$\\Theta$$, $$Z$$에 대하여 주변화(marginalize out) 함으로써 구할 수 있다. (이를 evidence 혹은 marginal likelihood라고 한다.)\r\n\r\n$$\r\n\r\n\\begin{aligned}\r\nP(W|\\alpha,\\beta) &= \\displaystyle\\int_{\\Phi}\\displaystyle\\int_{\\Theta}\\displaystyle\\sum_{Z}P(W,Z,\\Theta,\\Phi;\\alpha,\\beta)d\\Theta d\\Phi \\\\ &= \\displaystyle\\int_{\\Phi} P(\\Phi|\\beta) \\displaystyle\\int_{\\Theta} P(\\Theta|\\alpha) \\displaystyle\\sum_{Z} P(Z|\\Theta)P(W|Z,\\Phi) d\\Theta d\\Phi\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 3: Evidence</b>\r\n</figcaption>\r\n\r\n보통 현실 세계 문제에서, Evidence를 계산하는 것은 불가능한데 이를 보통 intractable하다고 표현한다. 왜냐하면 위 식에서 볼 수 있듯이, $$\\Theta$$와 $$\\Phi$$ 가 잠재 변수 Z에 대한 summation 내에 coupling 되어 있기 때문이다.\r\n\r\n> Z(topic assignment)에 대한 configuration이 $$K^{|M*N|}$$ 개 만큼 존재한다. 이처럼 베이지안 추론에서는 evidence의 존재가 LDA 같은 베이지안 모델을 어렵게 만드는 원인이라고도 볼 수 있다.\r\n\r\n하지만, 이런 상황에도 해결 방법이 존재한다.\r\n\r\n모델의 잠재 변수(엄밀하게는 파라미터)를 추정하기 위한 베이지안 추론에는 크게 두가지 방법론이 있다.\r\n\r\n1. Variational Inference\r\n2. Gibbs Sampling\r\n\r\n[베이지안 추론](/docs/research/bayesian-inference)에 대해서는 다룰 내용이 방대하므로 별도의 페이지에서 정리를 할 예정이고, 여기에서는 Gibbs Sampling 방식으로 설명을 이어나가도록 하겠다.\r\n\r\n## (Collapsed) Gibbs Sampling\r\n\r\nGibbs Sampling은 intractable한 분포를 가진 사후 확률 분포(posterior)를 추론하는데 사용되는 일반적인 접근법이다.\r\n\r\nLDA에서는 $$\\Phi$$와 $$\\Theta$$에 대한 사전 확률 분포(prior)가 conjugate하기 때문에, 결합 확률 분포로 부터 해당 확률 변수를 integrated out(=marginalized out)하여 계산하게 되는데, prior를 callapsing 한다고 하여 특별히 이것을 Collapsed Gibbs Sampling 이라고 부른다.\r\n\r\n즉, Collapsed Gibbs Sampling의 목표는 아래와 같이 Z에 대한 사후 확률 분포를 구하는 것이다.\r\n\r\n$$\r\n\\begin{aligned}\r\nP(Z|W;\\alpha,\\beta) = \\cfrac {P(Z,W;\\alpha,\\beta)} {P(W|\\alpha,\\beta)}\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 4: Collapsed Gibbs Sampling의 목표</b>\r\n</figcaption>\r\n\r\n먼저 위 식 4 우변의 분자는, 식 2 결합 확률 분포를 $$\\Phi$$와 $$\\Theta$$에 대하여 marginalize 하여 다음과 같이 정리해보자.\r\n\r\n$$\r\n\\begin{aligned}\r\nP(Z,W|\\alpha,\\beta) &= \\displaystyle\\int_{\\Theta}\\displaystyle\\int_{\\Phi}P(Z,W,\\Theta,\\Phi;\\alpha,\\beta)d\\Phi d\\Theta \\\\ &= \\displaystyle\\int_{\\Phi}\\displaystyle\\prod_{i=1}^{K} P(\\phi_i;\\beta) \\displaystyle\\prod_{j=1}^{M} \\displaystyle\\prod_{t=1}^{N} P(W_{j,t}|\\phi_{z_{jt}})d\\Phi \\cdot \\displaystyle\\int_{\\Theta} \\displaystyle\\prod_{j=1}^{M} P(\\theta_j;\\alpha) \\displaystyle\\prod_{t=1}^{N} P(Z_{j,t} | \\theta_j) d\\theta \\\\ &= ... \\\\ &=\r\n\\displaystyle\\prod_{j=1}^{M}\r\n\\cfrac {\\Gamma \\big(\\sum_{i=1}^{K} \\alpha_{i} \\big)}  {\\prod_{i=1}^{K} \\Gamma(\\alpha_{i})}\r\n\\cfrac {\\prod_{i=1}^K \\Gamma(n_{j,(\\cdot)}^i)+\\alpha_{i}} {\\Gamma(\\sum_{i=1}^{K} n_{j,(\\cdot)}^{i} + \\alpha_{i})}\r\n\r\n\\times\r\n\\displaystyle\\prod_{i=1}^{K}\r\n\\cfrac {\\Gamma \\big(\\sum_{r=1}^{V} \\beta_{r} \\big)}  {\\prod_{r=1}^{V} \\Gamma(\\beta_{r})}\r\n\\cfrac {\\prod_{r=1}^V \\Gamma(n_{(\\cdot), r}^i)+\\beta_{r}} {\\Gamma(\\sum_{r=1}^{V} n_{(\\cdot),r}^{i} + \\beta_{r})}\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 4: marginalized out</b>\r\n</figcaption>\r\n\r\n- $$n_{j,r}^{i}$$: $$j$$번째 문서에서 vocabulary상 id값이 $$r$$인 단어에, id값이 i인 토픽이 할당된 단어(토큰)의 갯수\r\n- $$n_{j,(\\cdot)}^{i}$$: $$j$$번째 문서에서, id값이 i인 토픽이 할당된 단어(토큰)의 총 갯수\r\n\r\n> 위 생략된 부분은 [위키피디아](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)에 디테일한 과정이 있으니 참고하기 바란다.\r\n\r\nGibbs Sampling은 사후 확률 분포를 직접 구하는 것이 아니라, 사후 확률 분포의 샘플(Z의 샘플)을 만들어 내는 것이 목표이기 때문에,\r\nZ에 대해서 불변한 값인 $$P(W,\\alpha,\\beta)$$를 무시하고, $$P(Z,W,\\Theta,\\Phi;\\alpha,\\beta)$$로부터 직접 Sampling Equation을 유도해 낼 수 있다.\r\n\r\n## Sampling Equation\r\n\r\n$$\r\n\\begin{aligned}\r\nP(Z_{(m,n)}|Z_{-(m,n)},W;\\alpha,\\beta)\r\n= \\cfrac {P(Z_{(m,n)}, Z_{-(m,n)},W,\\alpha,\\beta)} {P(Z_{-(m,n)}, W|\\alpha,\\beta)}\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 5: LDA sampling equation의 유도 - 1</b>\r\n</figcaption>\r\n\r\n- $$Z_{(m,n)}$$: $$m$$번째 문서의 $$n$$번째 단어에 대한 토픽을 나타내는 확률 변수\r\n- $$Z_{-(m,n)}$$: $$m$$번째 문서의 $$n$$번째 단어를 제외하고, 모든 단어에 대한 토픽을 나타내는 확률 변수\r\n\r\n위 식처럼 결합 확률 분포를 직접 구하는 것 대신, 조건부 확률을 통해 샘플을 만들어 낸다. 다시 말하면, Gibbs sampler는 각 잠재 변수의 좋은? 샘플을 만들어 내기 위해서, 그 외 다른 모든 잠재 변수는 현재 값으로 고정시키고 나서 특정 잠재 변수에 대한 조건부 확률을 계산하고 이 조건부 확률 분포로 부터 해당 잠재 변수 값을 샘플링 하게 된다.\r\n\r\n> 샘플링은 inverse transforming sampling 방식으로 구현한다.\r\n\r\n결과적으로, 만약 $$m$$문서의 $$n$$번째 단어의 토픽이 $$v$$로 할당된다라고 할 때, 그 확률은 다음과 같이 정리 된다.\r\n\r\n$$\r\n\\begin{aligned}\r\n\\color{blue} {\r\n  P(Z_{(m,n)} = v|Z_{-(m,n)}, W;\\alpha,\\beta)\r\n}\r\n&\\propto\r\nP(Z_{(m,n)}=v, Z_{-(m,n)}, W;\\alpha, \\beta)\r\n\r\n\\\\ &\\propto ...\r\n\r\n\\\\ &\\propto\r\n\\displaystyle\\prod_{i=1}^{K} \\Gamma \\bigg( n_{m,(\\cdot)}^{i} + \\alpha_{i} \\bigg)\r\n\\displaystyle\\prod_{i=1}^{K} \\cfrac {\\Gamma(n_{(\\cdot), v}^i)+\\beta_{v}} {\\Gamma(\\sum_{r=1}^{V} n_{(\\cdot),r}^{i} + \\beta_{r})}\r\n\r\n\\\\ &\\propto ...\r\n\r\n\\\\ &\\propto\r\n\\color{blue} {\r\n  \\bigg( n_{m,(\\cdot)}^{k,-(m,n)} + \\alpha_{k} \\bigg) \\cfrac { \\Gamma \\bigg( n_{(\\cdot),v}^{k,-(m,n)} + \\beta_{v} \\bigg) } {\\sum_{r=1}^{V}  n_{(\\cdot),r}^{k,-(m,n)} + \\beta_{r}}\r\n}\r\n\r\n\\end{aligned}\r\n$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식 5: LDA sampling equation의 유도 - 2</b>\r\n</figcaption>\r\n\r\n- $$n_{m,(\\cdot)}^{k, -(m,n)}$$: $$m$$번째 문서에서 id값이 $$k$$인 토픽(단, $$Z_(m,n)$$은 제외, 즉 현재 단어에 어떤 토픽이 할당되는지는 제외)이 할당된 단어(토큰)의 갯수\r\n\r\n> 위 생략된 부분은 [위키피디아](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)에 디테일한 과정이 있으니 참고하기 바란다.\r\n\r\n# Gibss Sampling 과정 요약\r\nLDA 모델 학습에 있어서 가장 중요한 식이 바로 위(파란색)에 유도되었다.\r\n\r\n실제 구현 레벨에서 위 식을 이용하는 방법은 다음의 과정을 따르게 된다. (간단히만 설명)\r\n> 향후 개인적으로 구현한 LDA 모델을 github에 올릴 예정이다.\r\n\r\n1. 모든 문서 내, 모든 단어(토큰) 마다 임의의 토픽을 할당한다.\r\n2. 모든 문서, 모든 단어를 하나씩 스캔하면서, 현재 단어에 할당된 토픽을 무효화 하고, 1 부터 K의 토픽 할당 했을 때의 조건부 확률 값(위의 파란색 식)을 계산한다.\r\n3. 2에서 구한 각 토픽 별 확률값을 누적 확률 분포로 표현하고, 이 누적 함수의 역함수를 구한다. \r\n4. 0 부터 1사이의 난수를 위 역함수의 입력으로하여 함수 값을 찾으면 이것이 현재 단어(토큰)의 $$Z$$ 값이 샘플링 된 것으로 생각할 수 있다.\r\n5. 다음 단어를 스캔하면서 위 2번 부터 다시 반복한다.\r\n\r\n> 위 3번, 4번 과정에서 설명한 부분이 앞서 언급한 inverse transform sampling 방식을 설명한 것인데 이는 별도의 페이지에서 추가로 섦명하도록 하겠다.\r\n\r\n<img width=\"400\" src=\"/assets/research/topic_modeling/lda/gibbs-sampling.png\" />\r\n<figcaption align=\"center\">\r\n  <b>그림3: Gibbs Sampling pseudo code</b>\r\n</figcaption>\r\n\r\n\r\n결론적으로, 과정을 반복하다보면 우리가 목표로 하는 잠재 변수에 대한 사후 확률 분포, $$P(Z|W;\\alpha,\\beta)$$, 가 점점 (데이터를 가장 잘 설명하는) 목표 분포에 수렴하게 되고, Gibbs Sampling에서는 실제 Z의 샘플을 만들었기 때문에, 각 단어에 어떤 토픽을 할당하는 것이 좋은지 에 대한 결과를 얻을 수 있다.\r\n\r\n<img width=\"400\" src=\"/assets/research/topic_modeling/lda/convergence.png\" />\r\n<figcaption align=\"center\">\r\n  <b>그림4: Convergence</b>\r\n</figcaption>\r\n\r\n위 그림은 임의로 가져온 그림이긴 한데, 각 점을 $$Z$$의 configuration을 나타내는 어떤 state라고 이해하면 좋을 것 같다.\r\n\r\n사실 개인적으로는 이 과정이 가장 핵심이라고 생각한다.\r\n그 이유는 현재 단어에 어떤 토픽이 할당 되는 것이 가장 그럴듯 할지에 대해 섦명하는 확률이 식 5인데 argmax를 취하여 할당 될 토픽을 결정하는 것이 아닌, 말 그대로 샘플링을 통해서 결정한다라는 부분을 어떻게 받아들여야 할지를 이해하는 것이 중요하기 때문이다. 이러한 방식이 Markov Chain Monte Carlo(MCMC) 메소드 라고 하는 것인데 이 부분에 대한 섦명은 마찬가지로 별도의 페이지에서 설명할 예정이다.\r\n\r\n> Gibbs Samping은 대표적인 Markov Chain Monte Carlo(MCMC) 알고리즘이다.\r\n\r\n\r\n위에 설명한 내용을 이해하기 위해서는 상당히 많은 배경지식 필요하기 때문에 상당 부분 설명이 생략 된 부분이 많이 있지만, 나름 핵심적인 사고 과정은 포함되어 있다고 생각한다. 조금씩 시간이 날 때마다 디테일한 부분을 업데이트 하도록 하겠다.\r\n","path":"00.research/topic-modeling/lda"},"__N_SSG":true}