{"pageProps":{"mappings":[{"header":{"label":"연구","path":"00.research","md":"","preview":true,"sub":[{"label":"베이지안 추론","path":"bayesian-inference","md":"","preview":true,"sub":[{"label":"Coin Tossing","path":"coin-tossing","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/coin-tossing/coin-tossing.jpg","snippet":"동전을 던졌을 때, 앞면이 나올 확률을 데이터에 기반하여 추정해보자","depth":2},{"label":"Curve Fitting","path":"curve-fitting","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/curve-fitting/curve-fitting.JPG","snippet":"가지 중요한 개념을 설명하기에 앞서, 간단한 회귀(Regression) 문제를 소개해 보도록 하겠다.","depth":2},{"label":"Gibbs Sampling","path":"gibbs-sampling","md":"","preview":true,"sub":[],"img":"/assets/study/inverse-transform-sampling/Inverse_Transform_Sampling_Example.gif","snippet":"Gibbs Sampling을 구현하기위해 사용한 Inverse Transform Sampling 기법을 소개하면서 실질적인 구현 방법을 먼저 소개하고, 이론적인 배경은 나중에 업데이트 할 예정이다.","depth":2},{"label":"Variational Inference","path":"variational-inference","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Inference는 [Bayeisan Inference](/docs/research/bayesian-inference)에서 적용되는 테크닉으로 개인적으로는 상당히 공부하기 어려웠던 것 중 하나여서 시간을 내어 정리해 보려고 한다.","depth":2}],"img":"/barcode.png","snippet":"Inference를 설명하기에 앞서 다음과 같은 순서로 각 개념을 이해하는 것이 중요하다.","depth":1},{"label":"가우시안 혼합 모델","path":"gaussian-mixture-model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"K-means Clustering","path":"k-means","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"K-means 알고리즘은 Gaussian Mixture Model의 특별한 경우이다. 그리고 EM 알고리즘의 Expectation 단계와 Maximazation 단계를 거쳐 학습하는 과정을 거친다.","depth":1},{"label":"Multi-Armed Bandit","path":"multi-armed-bandit","md":"","preview":true,"sub":[],"img":"/assets/research/multi-armed-bandit/mab.JPG","snippet":"여러대의 슬롯 머신이 있다고 하자. 그리고 일확천금을 위해서 어떤 사람이 슬롯 머신을 여기 저기서 당기고 있다. 이때 이 사람이 수익을 극대화 하는 방법이 있을까?","depth":1},{"label":"PageRank","path":"pagerank","md":"","preview":true,"sub":[],"img":"/assets/research/pagerank/pagerank.png","snippet":"상당히 직관적이고 간단하게 이해할 수 있는 개념이지만 그 이면을 들여다 보면 공부할 만한 사실들이 상당히 많이 있다. 그 중 중요하다고 생각하는 부분들에 대해서 소개하려고 한다.","depth":1},{"label":"추천 시스템","path":"recommendation-system","md":"","preview":true,"sub":[{"label":"컨텐츠 기반 알고리즘","path":"contents","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Matrix Factorization","path":"matrix-factorization","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Factorization은 추천 시스템에서 협업 필터링(Collaborative Filtering) 알고리즘에 속한다. 아이디어는 상당히 간단한데 User와 Item을 행과 열로 가진 Matrix 분해햐여 User와 Item을 low dimensional latent space에 사상 시키는 방법이다. 이를 위해 아랴와 같이 크게 두가지 방식으로 User-Item Matrix를 Decomposition 할 수 있다.","depth":2},{"label":"모델 기반 협업 필터링","path":"model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"neighbor","path":"neighbor","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"Stochastic Process","path":"stochastic-process","md":"","preview":true,"sub":[{"label":"디리클레 프로세스","path":"dirichlet-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"가우시안 프로세스","path":"gaussian-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"혹스 프로세스","path":"hawkes-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"포아송 프로세스","path":"poisson-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"Stochastic Process란, Random Variable(확률 변수) 혹은 function의 collection을 의미한다.","depth":1},{"label":"Singular Value Decomposition","path":"svd","md":"","preview":true,"sub":[],"img":"/assets/research/svd/axis.JPG","snippet":"Singular Value Decomposition (이하 SVD)는 Eigendecomposition의 일반화된 형태이므로 먼저 Eigendecompositon에 대해 정리해 본다.","depth":1},{"label":"Topic Model","path":"topic-modeling","md":"","preview":true,"sub":[{"label":"Adversarial-neural Event Model","path":"aem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Correlated Topic Model","path":"ctm","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Gaussain LDA","path":"glda","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Hierarchical Dirichlet Process","path":"hdp","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"LDA의 Non-parametric 버전으로 토픽 갯수 K를 지정하지 않아도 되는 더 일반적인 모델","depth":2},{"label":"Latent Dirichlet Allocation","path":"lda","md":"","preview":true,"sub":[],"img":"/assets/research/topic_modeling/lda/dist_desc.JPG","snippet":"LDA는 임의의 문서를 K개의 토픽 분포로 표현하고, 각 토픽은 V개의 단어 분포로 표현하는 모델이다.","depth":2},{"label":"Latent Event Model","path":"lem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"토픽 모델이 뭔지 정리해보자","depth":1},{"label":"Variational AutoEncoder","path":"variational-autoencoder","md":"","preview":true,"sub":[],"img":"/assets/research/variational-autoencoder/ae-vae.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"공부했던 것들 중에, 생각할 것들이 많았던 것들을 정리하고 있다.","depth":0},"side_bar":[{"label":"베이지안 추론","path":"bayesian-inference","md":"","preview":true,"sub":[{"label":"Coin Tossing","path":"coin-tossing","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/coin-tossing/coin-tossing.jpg","snippet":"동전을 던졌을 때, 앞면이 나올 확률을 데이터에 기반하여 추정해보자","depth":2},{"label":"Curve Fitting","path":"curve-fitting","md":"","preview":true,"sub":[],"img":"/assets/research/bayesian-inference/curve-fitting/curve-fitting.JPG","snippet":"가지 중요한 개념을 설명하기에 앞서, 간단한 회귀(Regression) 문제를 소개해 보도록 하겠다.","depth":2},{"label":"Gibbs Sampling","path":"gibbs-sampling","md":"","preview":true,"sub":[],"img":"/assets/study/inverse-transform-sampling/Inverse_Transform_Sampling_Example.gif","snippet":"Gibbs Sampling을 구현하기위해 사용한 Inverse Transform Sampling 기법을 소개하면서 실질적인 구현 방법을 먼저 소개하고, 이론적인 배경은 나중에 업데이트 할 예정이다.","depth":2},{"label":"Variational Inference","path":"variational-inference","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Inference는 [Bayeisan Inference](/docs/research/bayesian-inference)에서 적용되는 테크닉으로 개인적으로는 상당히 공부하기 어려웠던 것 중 하나여서 시간을 내어 정리해 보려고 한다.","depth":2}],"img":"/barcode.png","snippet":"Inference를 설명하기에 앞서 다음과 같은 순서로 각 개념을 이해하는 것이 중요하다.","depth":1},{"label":"가우시안 혼합 모델","path":"gaussian-mixture-model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"K-means Clustering","path":"k-means","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"K-means 알고리즘은 Gaussian Mixture Model의 특별한 경우이다. 그리고 EM 알고리즘의 Expectation 단계와 Maximazation 단계를 거쳐 학습하는 과정을 거친다.","depth":1},{"label":"Multi-Armed Bandit","path":"multi-armed-bandit","md":"","preview":true,"sub":[],"img":"/assets/research/multi-armed-bandit/mab.JPG","snippet":"여러대의 슬롯 머신이 있다고 하자. 그리고 일확천금을 위해서 어떤 사람이 슬롯 머신을 여기 저기서 당기고 있다. 이때 이 사람이 수익을 극대화 하는 방법이 있을까?","depth":1},{"label":"PageRank","path":"pagerank","md":"","preview":true,"sub":[],"img":"/assets/research/pagerank/pagerank.png","snippet":"상당히 직관적이고 간단하게 이해할 수 있는 개념이지만 그 이면을 들여다 보면 공부할 만한 사실들이 상당히 많이 있다. 그 중 중요하다고 생각하는 부분들에 대해서 소개하려고 한다.","depth":1},{"label":"추천 시스템","path":"recommendation-system","md":"","preview":true,"sub":[{"label":"컨텐츠 기반 알고리즘","path":"contents","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Matrix Factorization","path":"matrix-factorization","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Factorization은 추천 시스템에서 협업 필터링(Collaborative Filtering) 알고리즘에 속한다. 아이디어는 상당히 간단한데 User와 Item을 행과 열로 가진 Matrix 분해햐여 User와 Item을 low dimensional latent space에 사상 시키는 방법이다. 이를 위해 아랴와 같이 크게 두가지 방식으로 User-Item Matrix를 Decomposition 할 수 있다.","depth":2},{"label":"모델 기반 협업 필터링","path":"model","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"neighbor","path":"neighbor","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"Stochastic Process","path":"stochastic-process","md":"","preview":true,"sub":[{"label":"디리클레 프로세스","path":"dirichlet-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"가우시안 프로세스","path":"gaussian-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"혹스 프로세스","path":"hawkes-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"포아송 프로세스","path":"poisson-process","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"Stochastic Process란, Random Variable(확률 변수) 혹은 function의 collection을 의미한다.","depth":1},{"label":"Singular Value Decomposition","path":"svd","md":"","preview":true,"sub":[],"img":"/assets/research/svd/axis.JPG","snippet":"Singular Value Decomposition (이하 SVD)는 Eigendecomposition의 일반화된 형태이므로 먼저 Eigendecompositon에 대해 정리해 본다.","depth":1},{"label":"Topic Model","path":"topic-modeling","md":"","preview":true,"sub":[{"label":"Adversarial-neural Event Model","path":"aem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Correlated Topic Model","path":"ctm","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Gaussain LDA","path":"glda","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2},{"label":"Hierarchical Dirichlet Process","path":"hdp","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"LDA의 Non-parametric 버전으로 토픽 갯수 K를 지정하지 않아도 되는 더 일반적인 모델","depth":2},{"label":"Latent Dirichlet Allocation","path":"lda","md":"","preview":true,"sub":[],"img":"/assets/research/topic_modeling/lda/dist_desc.JPG","snippet":"LDA는 임의의 문서를 K개의 토픽 분포로 표현하고, 각 토픽은 V개의 단어 분포로 표현하는 모델이다.","depth":2},{"label":"Latent Event Model","path":"lem","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":2}],"img":"/barcode.png","snippet":"토픽 모델이 뭔지 정리해보자","depth":1},{"label":"Variational AutoEncoder","path":"variational-autoencoder","md":"","preview":true,"sub":[],"img":"/assets/research/variational-autoencoder/ae-vae.png","snippet":"","depth":1}]},{"header":{"label":"개발","path":"01.development","md":"","preview":true,"sub":[{"label":"엘라스틱서치","path":"elasticsearch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"사용했던 것들을 정리해보자.","depth":1},{"label":"개발 환경 구축","path":"env","md":"","preview":true,"sub":[{"label":"code-server","path":"code-server","md":"","preview":true,"sub":[],"img":"/assets/development/env/code-server/code-server.png","snippet":"+ ubuntu 20.04","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"명령어 기록","path":"etc","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"sh","depth":1},{"label":"하둡","path":"hadoop","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"java","path":"java","md":"","preview":true,"sub":[{"label":"자바의 비동기 기술","path":"async","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Future","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"쿠버네티스","path":"k8s","md":"","preview":true,"sub":[{"label":"쿠버네티스 설치","path":"installation","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"집에 놀고 있는 리눅스 머신에 K8S를 설치 해보자","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"메시지 브로커","path":"kafka","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"보내고, 처리하고, 삭제한다.","depth":1},{"label":"코틀린","path":"kotlin","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"루씬","path":"lucene","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"거의 Elasticsearch를 이용해서 프로젝트를 진행하지만, 검색이 필요한 경우 Lucene을 이용해서 개발하는 경우가 많았던 것 같다. JAVA에 Lucence 의존성만 추가하면 뭔가 가볍게 시작할 수 있었기 때문인데 점점 기능이 복잡해 질 수록 Elasticsearch가 얼마나 잘 만들어져 있는 것인가를 느끼고 있다. 그래도 Elasticsearch는 Lucence을 가져다 쓰는거니까 먼저 간단한 것 부터 정리해 볼 계획이다.","depth":1},{"label":"리액트","path":"react","md":"","preview":true,"sub":[{"label":"package.json","path":"package-json","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"dependencies vs devDependencies","depth":2},{"label":"react-snap으로 정적페이지 빌드","path":"react-snap","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Basic usage with create-react-app","depth":2},{"label":"GitHub Pages에 SPA","path":"spa-github-pages","md":"","preview":true,"sub":[],"img":"/assets/development/react/spa-github-pages/github-pages-404.JPG","snippet":"1. 문제 상황","depth":2}],"img":"/assets/development/react/react-app.png","snippet":"React 개발 환경 구축","depth":1},{"label":"스프링 부트","path":"spring-boot","md":"","preview":true,"sub":[{"label":"Spring Boot에서 HTTPS 적용","path":"https","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Certificate 만들기","depth":2},{"label":"Maven에서 Spring Boot 설정","path":"maven-support","md":"","preview":true,"sub":[],"img":"/assets/development/spring-boot/maven-support/multi-module.JPG","snippet":"Maven multi-module 프로젝트에서 Spring Boot Application을 Maven Dependency로 Import하기","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"웹플럭스","path":"webflux","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"","depth":0},"side_bar":[{"label":"엘라스틱서치","path":"elasticsearch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"사용했던 것들을 정리해보자.","depth":1},{"label":"개발 환경 구축","path":"env","md":"","preview":true,"sub":[{"label":"code-server","path":"code-server","md":"","preview":true,"sub":[],"img":"/assets/development/env/code-server/code-server.png","snippet":"+ ubuntu 20.04","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"명령어 기록","path":"etc","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"sh","depth":1},{"label":"하둡","path":"hadoop","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"java","path":"java","md":"","preview":true,"sub":[{"label":"자바의 비동기 기술","path":"async","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Future","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"쿠버네티스","path":"k8s","md":"","preview":true,"sub":[{"label":"쿠버네티스 설치","path":"installation","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"집에 놀고 있는 리눅스 머신에 K8S를 설치 해보자","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"메시지 브로커","path":"kafka","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"보내고, 처리하고, 삭제한다.","depth":1},{"label":"코틀린","path":"kotlin","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"루씬","path":"lucene","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"거의 Elasticsearch를 이용해서 프로젝트를 진행하지만, 검색이 필요한 경우 Lucene을 이용해서 개발하는 경우가 많았던 것 같다. JAVA에 Lucence 의존성만 추가하면 뭔가 가볍게 시작할 수 있었기 때문인데 점점 기능이 복잡해 질 수록 Elasticsearch가 얼마나 잘 만들어져 있는 것인가를 느끼고 있다. 그래도 Elasticsearch는 Lucence을 가져다 쓰는거니까 먼저 간단한 것 부터 정리해 볼 계획이다.","depth":1},{"label":"리액트","path":"react","md":"","preview":true,"sub":[{"label":"package.json","path":"package-json","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"dependencies vs devDependencies","depth":2},{"label":"react-snap으로 정적페이지 빌드","path":"react-snap","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Basic usage with create-react-app","depth":2},{"label":"GitHub Pages에 SPA","path":"spa-github-pages","md":"","preview":true,"sub":[],"img":"/assets/development/react/spa-github-pages/github-pages-404.JPG","snippet":"1. 문제 상황","depth":2}],"img":"/assets/development/react/react-app.png","snippet":"React 개발 환경 구축","depth":1},{"label":"스프링 부트","path":"spring-boot","md":"","preview":true,"sub":[{"label":"Spring Boot에서 HTTPS 적용","path":"https","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"Certificate 만들기","depth":2},{"label":"Maven에서 Spring Boot 설정","path":"maven-support","md":"","preview":true,"sub":[],"img":"/assets/development/spring-boot/maven-support/multi-module.JPG","snippet":"Maven multi-module 프로젝트에서 Spring Boot Application을 Maven Dependency로 Import하기","depth":2}],"img":"/barcode.png","snippet":"","depth":1},{"label":"웹플럭스","path":"webflux","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]},{"header":{"label":"아무거나 정리","path":"02.study","md":"","preview":true,"sub":[{"label":"Chi-Square Test","path":"chi-square-test","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"검정은 하나 이상의 카테고리에서 관측된 빈도와 기대되는 빈도가 통계적으로 유의하게 다른지 검증하는 기법으로, 카이 제곱 분포에 기초한 통계적 가설 검정 방법이다.","depth":1},{"label":"HTTPS와 공개 키 암호 방식","path":"crypto","md":"","preview":true,"sub":[],"img":"/assets/study/crypto/https.png","snippet":"그린 그림인지 모르겠지만 가장 쉽게 잘 설명해주신 것 같다.","depth":1},{"label":"NumPy","path":"numpy","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"np.array","depth":1},{"label":"P-value","path":"p-value","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"pandas","path":"pandas","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"PyTorch","path":"pytorch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"궁금해서 찾아본 것, 알고 있었는데 까먹고 있었던 것, 생각 날 때마다 정리해보자.","depth":0},"side_bar":[{"label":"Chi-Square Test","path":"chi-square-test","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"검정은 하나 이상의 카테고리에서 관측된 빈도와 기대되는 빈도가 통계적으로 유의하게 다른지 검증하는 기법으로, 카이 제곱 분포에 기초한 통계적 가설 검정 방법이다.","depth":1},{"label":"HTTPS와 공개 키 암호 방식","path":"crypto","md":"","preview":true,"sub":[],"img":"/assets/study/crypto/https.png","snippet":"그린 그림인지 모르겠지만 가장 쉽게 잘 설명해주신 것 같다.","depth":1},{"label":"NumPy","path":"numpy","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"np.array","depth":1},{"label":"P-value","path":"p-value","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"pandas","path":"pandas","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1},{"label":"PyTorch","path":"pytorch","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]},{"header":{"label":"토이 프로젝트","path":"03.project","md":"","preview":true,"sub":[{"label":"CNN 기반 형태소 분석기","path":"cnn-morph","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}],"img":"/barcode.png","snippet":"","depth":0},"side_bar":[{"label":"CNN 기반 형태소 분석기","path":"cnn-morph","md":"","preview":true,"sub":[],"img":"/barcode.png","snippet":"","depth":1}]}],"post":"# PageRank\r\n\r\nPageRank는 상당히 직관적이고 간단하게 이해할 수 있는 개념이지만 그 이면을 들여다 보면 공부할 만한 사실들이 상당히 많이 있다. 그 중 중요하다고 생각하는 부분들에 대해서 소개하려고 한다.\r\n\r\n<img width=\"400\" src=\"/assets/research/pagerank/pagerank.png\" />\r\n<figcaption align=\"center\">\r\n  <b>그림1: PageRank (출처: 위키피디아)</b>\r\n</figcaption>\r\n\r\ngraph theory에서 graph상에서 node의 증요도를 측정하는 것은 상당히 중요한 부분이다. 이것을 centrality를 측정한다라고 하는데, PageRank는 웹이라는 거대한 그래프 상에 존재하는 페이지 하나하나를 node로 하여 node의 eigenvector centrality를 측정하는 방식이라고 할 수 있다. 이 부분에 대해 설명하기 전에 먼저 PageRank 자체에 대해 알아보기로 하자.\r\n\r\nPageRank는 다음과 같은 가정으로 웹페이지의 중요도를 계산한다.\r\n\r\n1. 다른 페이지로 이동할 수 있는 link를 많이 포함하고 있는(outbound link가 많은) 페이지는 상대적으로 중요도가 낮다\r\n   - 예를 들면, sitemap을 담고 있는 페이지나, 포털과 같은 페이지는 다른 페이지로 건너 가기 위한 수단이 되는 페이지이므로 상대적으로 중요한 정보를 담고 있지 않다고 가정한다.\r\n2. 다른 페이지의 link로 부터 유입되기 쉬운(inbound link가 많은) 페이지는 상대적으로 중요도가 높다.\r\n   - 전문적인 지식을 담고 있는 블로그나 중요한 뉴스 기사는 다른 페이지로부터 참조될 소지가 많다.\r\n\r\n이러한 가정을 담은 식을 아래와 같이 표현 할 수 있다.\r\n\r\n$$\r\nPR(u) = \\sum_{v \\in B_u} \\frac{PR(v)}{L(v)},\r\n$$\r\n\r\n- $$PR(u)$$: 페이지 u의 중요도 (u의 페이지랭크 값)\r\n- $$B_u$$: 페이지 u로 link를 가진 모든 페이지 집합\r\n- $$L(v)$$: 페이지 v의 outbound link 수\r\n\r\n따라서 페이지 u의 중요도, $$PR(u))$$는 페이지 u로의 link를 가지고 있는 모든 페이지 v의 중요도, $$PR(v))$$의 합산으로 (단, $$L(v)$$로 나누는 방식으로 패널티를 주어) 표현한다.\r\n\r\n하지만 PageRank의 실제 식은 다음과 같은데,\r\n\r\n$$\r\nPR(p_i) = \\frac{1-d}{N} + d \\sum_{p_j \\in M(p_i)} \\frac{PR (p_j)}{L(p_j)}\r\n$$\r\n\r\n- $$N$$: 전체 페이지 수\r\n\r\n<figcaption align=\"center\">\r\n  <b>식1: PageRank Equation</b>\r\n</figcaption>\r\n\r\n이는 페이지 $$p_i$$의 중요도, $$PR(p_i)$$ 값은 $$\\cfrac 1 N$$과 $$\\sum_{p_j \\in M(p_i)} \\frac{PR (p_j)}{L(p_j)}$$을 $$ 1-d : d $$의 비율로 가중합 하여 계산하였다.\r\n\r\n어쩌면 이 부분 때문에 PageRank가 이론적으로 굉장히 탄탄한 배경을 가지고 있다고 볼 수 있는데, 좀 더 자세한 해석을 위해 위 식을 다음과 같이 행렬 표현식으로 바꾸어 보자.\r\n\r\n$$\r\n\\small{\r\n    \\begin{bmatrix}\r\n    PR(p_1)^{(i+1)} \\\\\r\n    PR(p_2)^{(i+1)} \\\\\r\n    \\vdots \\\\\r\n    PR(p_N)^{(i+1)}\r\n    \\end{bmatrix} =\r\n    \\begin{bmatrix}\r\n    {(1-d)/ N} \\\\\r\n    {(1-d) / N} \\\\\r\n    \\vdots \\\\\r\n    {(1-d) / N}\r\n    \\end{bmatrix}\r\n    + d\r\n    \\begin{bmatrix}\r\n    \\ell(p_1,p_1) & \\ell(p_1,p_2) & \\cdots & \\ell(p_1,p_N) \\\\\r\n    \\ell(p_2,p_1) & \\ddots &  & \\vdots \\\\\r\n    \\vdots & & \\ell(p_i,p_j) & \\\\\r\n    \\ell(p_N,p_1) & \\cdots & & \\ell(p_N,p_N)\r\n    \\end{bmatrix}\r\n    \\begin{bmatrix}\r\n    PR(p_1)^{(i)} \\\\\r\n    PR(p_2)^{(i)} \\\\\r\n    \\vdots \\\\\r\n    PR(p_N)^{(i)}\r\n    \\end{bmatrix}\r\n}\r\n$$\r\n\r\n- $$PR(p)^{i}$$: i번째 iteration에서의 페이지 p의 패이지랭크 값\r\n- $$\\ell(p_i, p_j)$$: 페이지 j에서 페이지 j로의 outbound link 갯수 / 페이지 j의 총 outbound link 갯수 $$\\bigg(\\sum_{i = 1}^N \\ell(p_i,p_j) = 1\\bigg)$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식2: PageRank Equation의 행렬 표현</b>\r\n</figcaption>\r\n\r\n위와 같이 표현 했을 때 알 수 있는 중요한 사실은,\r\n\r\n$\r\n\\mathbf{R} =\r\n\\begin{bmatrix}\r\nPR(p_1) \\\\\r\nPR(p_2) \\\\\r\n\\vdots \\\\\r\nPR(p_N)\r\n\\end{bmatrix}\r\n$ 은 eigenvector 라는 사실이다.\r\n\r\n그 이유는, 위 식 2를 다시 아래와 같이 정리할 수 있다.\r\n\r\n$$\r\n\\begin{aligned}\r\n\\mathbf{R}^{(i+1)}\r\n&=\r\n\\frac{1-d}{N} \\mathbf{1} + d \\cdot \\mathcal{A} \\cdot \\mathbf{R}^{(i)}\r\n\\\\ &=\r\n\\left(\\frac{1-d}{N} \\mathbf{E} + d \\cdot \\mathcal{A}  \\right)\\mathbf{R}^{(i)} =: \\widehat{ \\mathcal{A}} \\cdot \\mathbf{R}^{(i)}\r\n\\end{aligned}\r\n$$\r\n\r\n- $$\\mathbf{1}$$: 1로 이루어진 column vector\r\n- $$\\mathcal{A}$$: Adjecent Matrix (이 경우 각 컬럼의 합이 1이므로 stochastic matrix)\r\n- $$\\mathbf{E}$$: 단위 행렬 (단위 행렬은 당연히 stocahsitic matrix)\r\n  - $$\\mathbf{R}$$ 벡터를 propbability distribution로 생각하기 때문에, $$\\mathbf{E} \\cdot \\mathbf{R} = \\mathbf{1}$$\r\n\r\n<figcaption align=\"center\">\r\n  <b>식3: PageRank Equation의 행렬 표현</b>\r\n</figcaption>\r\n\r\n$$\r\n\\mathbf{Ax} = {\\lambda}\\mathbf{x}\r\n$$\r\n\r\n- $$\\mathbf{x}$$: eigenvector\r\n- $$\\lambda$$: eigenvalue\r\n\r\n<figcaption align=\"center\">\r\n  <b>식4: eigenvecotor와 eigenvalue</b>\r\n</figcaption>\r\n\r\n결론적으로 식 3은 식 4의 형태를 가지게 되고, 인접 행렬이 변형된 $$\\mathcal{A}$$ 행렬이 transition propbablity의 성격을 갖는다면, 즉 stochastic probability라면 eigenvalue가 단위 행렬 형태로 갖게 된다고 생각 할 수 있기 때문에, 패이지랭크 벡터 $$\\mathbf{R}$$은 식 2에서 표현한 것처럼 iterative하게 계산 될 수 있다. 이것을 power iteration(혹은 power method)라 한다.\r\n\r\n특히 인접 행렬이 변형된 $$\\mathcal{A}$$ 행렬을 살펴보면, 자기 자신으로 transition 될 확률을 최소한 $$\\cfrac {1-d} N$$ 만큼 보장 받게 된다. 이것은 damping factor라고 불리는 $$d$$라는 값 덕분에 웹사이트를 돌아다니는 random suffer가 어떤 노드에 정착해서 빠져나올 수 없는 상황을 방지하게 되는 역할을 한다.\r\n\r\n여기서 잠깐 PageRank를 좀 더 직관적으로 생각해 본다면, 웹서핑을 하는 사람이 랜덤으로 페이지에 있는 링크를 클릭하고, 클릭하고, ..., 클릭하고 를 무한히 반복한 다고 할때, 많이 방문하게 되는 페이지가 중요도가 높은 페이지라고 생각할 수 있는데, 이것은 인터넷에 존재하는 각 페이지들을 state로 하는 Markov Chain으로 볼 수 도 있다.\r\n\r\nMarkov Chain 관점에서 PageRank를 설명할 때 damping factor의 역할은, 각 페이지들을 state로 하는 Markov Chain에 erogodic한 속성을 부여해줌으로써, 각 노드의 중요도로 표현되는 PageRank 값은 아래와 같은 balance equestion을 통해 limiting distribuiton을 가진 벡터로 계산 될 수 있다.\r\n\r\n$$\r\n\\pi = \\pi P\r\n$$\r\n\r\n- $$\\pi$$: limiting probabilty\r\n- $$P$$: Markov Chain (transition probability 행렬)\r\n","path":"00.research/pagerank","visitors":{"today":0,"total":55}},"__N_SSG":true}